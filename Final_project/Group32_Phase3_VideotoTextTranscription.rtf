{\rtf1\ansi\ansicpg1252\cocoartf2638
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red49\green49\blue49;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c25098\c25098\c25098;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl313\sa213\partightenfactor0

\f0\fs29\fsmilli14667 \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 Yash:\ulnone  Hello all, this is group 32, with myself Yash and my teammates Harsh, Carter, and Ritwik presenting to you the Final Phase of the HCDR Project. In this presentation, we will go through the Overview in terms of the 4P's of the Phase 3, followed by our approach, then the results and finally the Conclusion and Scope. In the phase 2, we performed feature engineering, selection, analyses and hyperparameter tuning and for the current phase, we transitioned to creating an Artificial Neural Network. Deciding upon the size and layer counts of the multilayer perceptron model was a problem that we tackled in this phase of the project. As a scope to this project, we aim to gain better insights and implement bagging and boosting methods to get a better accuracy of the model.\
\ul Carter\ulnone : The goal of this project is to create an effective classification system for determining a client's financial ability to pay their debts. In order to find the best solution, we will investigate a variety of machine learning methods, from traditional machine learning to deep learning, feature engineering, and hyperparameter searches. Our first attempt was with three algorithms: logistic regression, Gaussian Na\'efve Bayes, random forest. In the next phase, we performed hyperparameter searches and feature engineering in order to increase the model\'92s accuracy. Finally, an artificial neural network was implemented, which further increased the accuracy of the model.\
\ul Harsh\ulnone : The validation accuracies from the first phase were around 91.8% for both the logistic regression and the random forest classifiers, while the Gaussian Na\'efve Bayes classifier was significantly lower at 13.7%. With the implementation of hyperparameters, the accuracy of the logistic regression classifier virtually stayed the same, while the random forest classifier\'92s accuracy increased to 98%. There were two neural networks implemented in Phase 3: one with 18 features, and one with 6. The neural network with 18 features gave an accuracy score of 91.8%, which was the most accurate classifier that we implemented. However, the one with 6 features was not quite as accurate, only having an accuracy score of 10%.\
\ul Ritwik\ulnone :
\fs53\fsmilli26667 \cf3  
\fs29\fsmilli14667 \cf2 Our model predicted the data very well at all steps in the project. The neural network ended up being the most accurate model that we implemented. Our final submission to Kaggle reached a public score of 0.72209. For future work, we plan to work on neural networks further to produce better parameters. We also want to perform more visualization functions to improve the readability to analyze more trends, implement more bagging and boosting methods to try to get a better result.\
\'a0\
}